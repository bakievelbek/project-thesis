\chapter{Related Work}


\section{Introduction}
This chapter presents an overview of existing methods for reconstructing damaged or corrupted audio signals, covering both classical signal processing techniques and recent machine learning approaches. Special attention is given to deep learning models based on generative adversarial networks (GANs), which have shown strong performance in enhancing the perceptual quality of speech. The discussion includes the widely used classical Wiener filter, early deep neural network models, and modern GAN-based solutions such as MetricGAN.

Emphasis is placed on the comparison between classical and advanced machine learning approaches, with a focus on their strengths, limitations, and practical applicability for real-world speech restoration tasks. This review establishes the context for selecting MetricGAN as the core of the current system, as well as the evaluation strategies used to measure performance.


\section{Classical Baseline Methods}

In the work of Hilal H. Nuha et al.~\cite{nuha-noise-reduction}, Wiener filtering is applied as a classical technique for speech enhancement under conditions of strong additive noise. The study evaluates the effectiveness of Wiener filtering by adding controlled levels of Gaussian noise to clean speech and then applying the filter to reduce noise. Performance is assessed using the mean squared error (MSE) between the filtered and original speech signals across a range of low signal-to-noise ratios (SNRs). The results indicate that Wiener filtering can noticeably reduce background noise and improve the clarity of speech, particularly in the non-speech regions. However, the method becomes less effective as noise levels increase and SNR decreases, highlighting its limitations in highly adverse or rapidly changing noise environments. These findings confirm that, although Wiener filtering remains a practical and efficient baseline, it faces challenges when applied to speech signals with very low SNR or dynamic noise conditions.

\vspace{1em}

A more advanced variation of the Wiener filter was introduced by Wageesha Manamperi et al.~\cite{gmm-manamperi}, who developed a multi-stage speech enhancement method based on Gaussian Mixture Models (GMMs) for low SNR environments. This technique models the power spectral densities of both speech and noise using GMMs, enabling adaptive noise suppression under non-stationary conditions. The enhancement process employs a parametric Wiener filter applied in multiple stages, where noise estimates are refined iteratively using GMM mean vectors. Evaluation on the TIMIT dataset, with various noise types and levels, demonstrates that the GMM-based approach achieves higher speech quality (PESQ) and intelligibility (STOI) compared to traditional Wiener filtering. The results highlight the effectiveness of combining classical filtering with statistical modeling, bridging the gap between conventional and modern machine learning-based speech restoration methods.

\section{Deep Learning Approaches}


In the work by Saleem et al.~\cite{saleem2018dnnlw}, a DNN-based supervised learning approach was proposed to enhance speech corrupted by speech-babble noise, one of the most challenging types of background noise. The method, called DNN-LW, combines a deep neural network with a less aggressive Wiener filtering layer. The DNN is trained to estimate the magnitude spectrums of clean speech and noise, and the Wiener filter is applied as a post-processing step to generate the final enhanced output. The system was evaluated using the Noizeus dataset at various SNR levels from –10dB to +10dB. Objective metrics such as PESQ,  speech distortion (SIG), residual noise (BAK), STOI, and SDR were used. Results showed that the proposed method outperformed baseline techniques like standard Wiener filtering, log minimum mean square error (logMMSE), and spectral subtraction (SS), delivering higher intelligibility and better perceptual quality in difficult noise conditions.

\vspace{1em}

In the study by Morrone et al.~\cite{morrone2021audiovisual}, the authors proposed a deep learning-based framework for audio-visual speech inpainting, which restores missing parts of speech signals using both the audio context and visual information from face landmarks. Their model used log-magnitude spectrograms and a stack of Bi-directional Long-Short Term Memory (BLSTM) layers to predict the missing time-frequency tiles. Additionally, they explored a multi-task learning (MTL) strategy that combines speech inpainting with phone recognition. The method was evaluated using standard metrics such as STOI and PESQ, Phone Error Rate (PER), and L1 loss. Results showed that the audio-visual model significantly outperformed audio-only baselines, especially on longer gaps up to 1600 ms. Adding visual information notably improved intelligibility and quality, demonstrating that vision is valuable in restoring speech with large missing segments. In contrast, TDNN-based approach focuses on short gaps, relying only on the audio context and using its ability to model local temporal structure efficiently.

\vspace{1em}

In the work of Schreibman et al.~\cite{schreibman2024frann}, a deep neural network called FRANN (Feature Restoration Additive Neural Network) was proposed to restore speech features that were degraded by traditional speech enhancement techniques. Their system was designed to follow a standard Wiener filter-based noise suppression module, aiming to improve speech intelligibility without increasing the noise level. The model combined convolutional layers with LSTM units in a U-Net structure and was trained on LibriMix data mixed with WHAM! noise. The evaluation was performed using scale invariant signal-to-distortion ratio (SI-SDR), PESQ, SDR, and signal-to-distortion ratio (SNR) metrics. Results showed that FRANN significantly improved SI-SDR and PESQ scores compared to the baseline system, demonstrating its effectiveness in restoring lost speech features in low-latency settings without affecting the residual noise.

\vspace{1em}

A notable example is the work by Marafioti et al.\cite{marafioti}, who proposed a DNN architecture for audio inpainting based on the surrounding time-frequency context. Their model processes the STFT representation of the audio and predicts the missing time–frequency coefficients by learning from the available context. The architecture combines convolutional layers and fully connected layers to effectively extract local and global features from the reliable parts of the signal. Two strategies were explored: reconstructing the complexe-valued TF coefficients directly and reconstructing only the magnitude spectrum with separate phase estimation. The model focusing on magnitude reconstruction demonstrated better results, outperforming traditional methods like linear predictive coding (LPC) in restoring naturalness and continuity of the audio signal.

\vspace{1em}

In the work of Sun et al.~\cite{sun2021rnn}, a recurrent neural network (RNN)-based speech enhancement method was developed for application in binaural hearing aid systems. The proposed architecture utilizes gated recurrent units (GRUs) to process binaural signals, extracting 55-dimensional feature vectors from each ear, including Bark-frequency cepstral coefficients and other auditory features. Separate GRU networks are trained on the signals from each ear before concatenation through a fully connected layer, allowing the model to preserve binaural cues. The method aims to achieve a balance between noise suppression and speech intelligibility, while maintaining low computational complexity suitable for real-time implementation on embedded devices and smartphones. Evaluation was conducted on datasets simulating various noisy environments, and the proposed method was compared to deep learning and statistical-model-based baselines such as rnnoise and FSGJMAP. Objective measures such as PESQ, STOI, and SNR showed that the RNN-based method provides a good balance between speech quality and intelligibility, making it suitable for hearing aid systems with limited resources.



\section{GAN-based Models}

In the work of Hung et al.~\cite{hung2024integrating}, a modern speech enhancement system was proposed for hearing aid applications by integrating noise classification and deep learning-based restoration. The system first employs a convolutional neural network (CNN) to classify environmental noise types from MFCC features with an average recognition accuracy of 92\%. Once the noise category is identified, the system uses a conditional Generative Adversarial Network (cGAN) based on the StarGAN architecture to enhance speech signals, conditioning the generator on the identified noise class. Experiments conducted on the VCTK dataset mixed with ten types of environmental noise at various SNR levels demonstrated that the StarGAN model effectively suppresses a wide range of noises. Objective evaluation using PESQ, STOI, and Mel-Cepstral Distortion (MCD) metrics showed that the system achieves lower distortion and improved speech quality compared to CycleGAN-based baselines, although some degradation of the speech signal itself was observed. These results show that combining noise type recognition with GAN-based enhancement can improve speech quality in real hearing aid systems.

\vspace{1em}

In the work of Phan et al.~\cite{phan2021selfattention}, the authors proposed a self-attention generative adversarial network (SASEGAN) for speech enhancement. Their approach improves on the original SEGAN by adding self-attention layers, which help the network better capture long-range dependencies in audio signals. The generator in SASEGAN uses an encoder-decoder structure with convolutional layers, and self-attention is applied at different points to improve how the model focuses on important parts of the input. Experiments on the Voice Bank dataset with various noise types and levels showed that SASEGAN achieves better results than the SEGAN baseline on objective measures such as PESQ, STOI, CSIG, CBAK, and COVL. These results suggest that adding self-attention helps GAN-based models produce clearer and more natural enhanced speech, even in challenging noisy environments.


\vspace{1em}

In the study by Xu et al.~\cite{xu2022vsegan}, a novel visual speech enhancement generative adversarial network (VSEGAN) was proposed to improve speech quality in noisy environments by integrating visual information from video frames. The VSEGAN framework combines a multi-layer feature fusion convolutional generator with a discriminator, both trained adversarially. The generator utilizes both the spectrogram of noisy speech and visual features extracted from facial video frames, processing these streams through separate audio and video encoders, followed by a fusion and decoding stage. The architecture is designed to effectively merge audio and visual cues, leveraging the invariance of visual speech properties to acoustic noise.

Training was performed on the GRID and TCD-TIMIT datasets with 12 types of real-world noise and variable SNRs, using PESQ and STOI as evaluation metrics. Experimental results demonstrated that VSEGAN outperformed traditional audio-only GAN models such as SEGAN, as well as other recent audio-visual speech enhancement methods, in both objective quality and intelligibility measures. These results show that using visual information with GANs can make speech enhancement much stronger, especially in difficult, noisy situations.

\vspace{1em}

In the work of Liu et al.~\cite{liu2020cpgan}, a Context Pyramid Generative Adversarial Network (CP-GAN) was introduced for speech enhancement. The main innovation of CP-GAN is its use of a densely-connected feature pyramid generator, which combines features from different levels to better capture both local and global information in speech signals. To further improve performance, the model uses a dynamic context granularity discriminator, which includes both a global discriminator (for whole audio) and a local discriminator (for random segments). This approach allows the model to remove noise more effectively, even when it is unevenly distributed across the audio.

CP-GAN was evaluated on the Voice Bank and Librispeech datasets, using common metrics such as segSNR, STOI, PESQ, and MOS-based measures. Results showed that CP-GAN outperforms previous GAN-based methods and also helps improve the results of speech recognition and speaker verification systems when using the enhanced audio. These findings suggest that combining hierarchical context information with GAN-based models can lead to more robust and high-quality speech enhancement.

\vspace{1em}

In the study by Faraji et al.~\cite{faraji2020afpgan}, a GAN-based speech enhancement system was introduced that uses audio fingerprinting features to improve both efficiency and performance. Instead of relying only on common features like the STFT or MFCC, the authors proposed combining MFCC with normalized spectral subband centroids (NSSC) and their temporal derivatives to create a compact feature set called AFPC. This feature set captures both the energy and the structure of speech formants, providing more robust and informative input for the GAN model. Experiments on the LibriSpeech dataset with various noise types and SNRs showed that the MFCC+NSSC combination achieved the best or near-best results in terms of PESQ, STOI, and SDR, while also significantly reducing memory use, training time, and model size compared to larger feature sets. These results suggest that combining audio fingerprinting features with GAN-based speech enhancement can make such systems more practical for real-time and embedded applications.


\section{Conclusion and Analysis}

The literature shows clear progress in speech signal restoration, moving from traditional signal processing methods to advanced deep learning and generative models. Classical approaches like the Wiener filter are simple and work well for steady noise, but often fail when noise is strong or changes quickly. Adding statistical models, such as GMM-based Wiener filters, helps adapt to some changing conditions but still struggles with very dynamic noise.

Deep learning has brought major improvements. DNN-based and hybrid systems, which combine neural networks with classical filters, are better at handling difficult noises and improve how speech sounds. Systems that use both audio and visual signals go even further, especially when large parts of the speech signal are missing.

Recent GAN-based models represent the latest developments. They learn more flexible and perceptually meaningful features, and new ideas like conditional GANs or special feature sets have made them practical for real-world use in hearing aids and phones.

However, each method has limitations. Classical models are fast and lightweight but less effective in tough situations. Deep learning and GAN-based approaches give better results but usually need large datasets and more computing power. GANs also face issues like training difficulty and the need for better ways to measure results.

The trend is toward data-driven models that directly improve how speech sounds to people, across many types of noise and damage. For these reasons, MetricGAN is chosen in this work, as it combines the advantages of GANs with the direct optimization of perceptual speech quality, addressing the main challenges found in previous research.